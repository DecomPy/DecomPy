

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Reinforcement Learning Ideas &mdash; DecomPy 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/extend_rtd.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="prev" title="Reinforcement Learning" href="1_ReinforcementLearning-Zach.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> DecomPy
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../1_Installation/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Design Decisions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../1_DesignAndArchitecture/index.html">Design and Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2_DataGathering/index.html">Data Gathering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_Database/index.html">Database</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_BinaryLifting/index.html">Binary Lifting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_UsingAnExistingDecompiler/index.html">Using an Existing Decompiler</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Machine Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="1_ReinforcementLearning-Zach.html">Reinforcement Learning</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Reinforcement Learning Ideas</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#applying-what-i-have-learned">Applying What I have Learned</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DecomPy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Design Decisions</a> &raquo;</li>
        
          <li><a href="index.html">Machine Learning</a> &raquo;</li>
        
      <li>Reinforcement Learning Ideas</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Documentation/3_DesignDecisions/6_MachineLearning/2_RL_Methods.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="reinforcement-learning-ideas">
<h1>Reinforcement Learning Ideas<a class="headerlink" href="#reinforcement-learning-ideas" title="Permalink to this headline">¶</a></h1>
<div class="section" id="applying-what-i-have-learned">
<h2>Applying What I have Learned<a class="headerlink" href="#applying-what-i-have-learned" title="Permalink to this headline">¶</a></h2>
<div class="section" id="modelling-the-world">
<h3>Modelling the world<a class="headerlink" href="#modelling-the-world" title="Permalink to this headline">¶</a></h3>
<p>If we choose to go with RL learning, we have to choose how to model our problem.
There are 4 components that we would have to decide when modelling our problem.
These are:
S  - The state
A  - The set of actions which can be chosen at a given point
R  - The reward from performing action ‘a’ from set A
S’ - The new state after action ‘a’</p>
<p>I propose the following environment:
S  - The current “state” of the program which starts from freshly raised LLVM
A  - The set of optimization passes which we have written to be used
R  - The delta between the current similarity of the program to the original</p>
<blockquote>
<div>and the similarity after applying the optimization pass, action ‘a’, from
our set of optimizations, “A”</div></blockquote>
<p>S’ - The new state of the program after the optimization</p>
</div>
<div class="section" id="what-type-of-rl">
<h3>What type of RL<a class="headerlink" href="#what-type-of-rl" title="Permalink to this headline">¶</a></h3>
<p>In order to use the model I mention in the previous section, the algorithm we choose
would need to be able to handle problems that are “episodic” in nature. That is to say,
each time that we start learning, we pick a single program (llvm module, function, whatever)
and perform the learning on that. Then we use the new information learned from that
to improve results on the next section of code when we begin again. (Or do some level
of hybrid batch based learning where we are slightly off policy as we learn a large batch
and perform computations based on the average result. This could cause faster learning
by smoothing the results. This is why most neural nets used batch based learning)</p>
<p>I concluded that the best choice of RL learning would be a model free on-policy
based algorithm which is episodic in nature and has a finite space horizon. I also noted
that it would be nice to find a learning model which is able to learn well from
similarities amongst states. Data efficiency is less of an issue as I believe we
have a good amount of data.</p>
<p>The following are models which roughly fit this criteria
* Temporal Difference Learning
* Temporal Difference Learning with Back Prop
* Monte Carlo learning</p>
</div>
<div class="section" id="modeling-the-world-part-2">
<h3>Modeling the World - Part 2<a class="headerlink" href="#modeling-the-world-part-2" title="Permalink to this headline">¶</a></h3>
<p>After further reading, and based on a discussion during a meeting with our project
sponsor, I came up with another model which I believe would be much more effective
and possible given our time frame. Essentially it is as follows:</p>
<p>S  (unchanged) - The current state of the program
R  (unchanged) - The delta representing improvement in similarity
S’ (unchanged) - The new state of the program after the action
A  (changed)   - This part is more complicated…. see below</p>
<p>The action part of the RL agent in our case is the real uncertainty in this project.
Whatever actions we provide are the actions that the ML agent will be able to take to
modify the code. Too much choice and we are left with incorrect code most of the time
and an almost impossible to train model. Too much constiction and it will be unable to
learn anything of value. The real issue is figuring out how to provide the freedom to
manipulate the program without losing semantics from the program during the process.
Anything we have come up with thus far is based around constricting its changes through
hand written rules – rules that in my opinion counteract what we set out to do. Our
sponsor’s goal is to avoid having to write rules for a specific compiler. Sure, writing
general rules and letting an ML agent decide which apply is a way to do this, but the
true aim in my opinion should be to avoid writing the rules in general. To this aim
I have the following solution.</p>
<p>Rather than provide rules that allow the ML agent to make passes through the code,
we can go back to the idea of free text based manipulation (or an LLVM module equivalent
there of). We would provide a database of short patterns broken down into basic equivalency
classes. Ignoring how we would make this database for now, if we had such a database,
providing the ML Agent freedom while mainting semantics is easy. Simply allow the
agent to swap out any values interchangably between the equivalency classes.</p>
<p>To generate this “database of equivalency classes”, we could put clang (or more specifically,
“opt” which is the part of clang that handles llvm optimization) to work. Since opt
is capable of specifically targeting passes, we can take a simple piece of unoptimized
llvm and generate as many equivalent versions of it as we desire (well, as many as there
are different opt passes which will actually result in a distinct version of the code).
By doing this, we ensure that the decompilation process will maintain semantic correctness,
but still give it a wide range of freedom.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="1_ReinforcementLearning-Zach.html" class="btn btn-neutral" title="Reinforcement Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Joshua Goralczyk, Salman Ahmed, Vatricia Edgar, YiZhuang Garrard, Zachary Monroe

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>