

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Reinforcement Learning Ideas &mdash; DecomPy 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/extend_rtd.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="prev" title="Reinforcement Learning" href="1_ReinforcementLearning-Zach.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> DecomPy
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../1_Installation/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Design Decisions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../1_DesignAndArchitecture/index.html">Design and Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2_DataGathering/index.html">Data Gathering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_Database/index.html">Database</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_BinaryLifting/index.html">Binary Lifting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_UsingAnExistingDecompiler/index.html">Using an Existing Decompiler</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Machine Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="1_ReinforcementLearning-Zach.html">Reinforcement Learning</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Reinforcement Learning Ideas</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#applying-what-i-have-learned">Applying What I have Learned</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DecomPy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Design Decisions</a> &raquo;</li>
        
          <li><a href="index.html">Machine Learning</a> &raquo;</li>
        
      <li>Reinforcement Learning Ideas</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Documentation/3_DesignDecisions/6_MachineLearning/2_RL_Methods.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="reinforcement-learning-ideas">
<h1>Reinforcement Learning Ideas<a class="headerlink" href="#reinforcement-learning-ideas" title="Permalink to this headline">¶</a></h1>
<div class="section" id="applying-what-i-have-learned">
<h2>Applying What I have Learned<a class="headerlink" href="#applying-what-i-have-learned" title="Permalink to this headline">¶</a></h2>
<div class="section" id="modelling-the-world">
<h3>Modelling the world<a class="headerlink" href="#modelling-the-world" title="Permalink to this headline">¶</a></h3>
<p>If we choose to go with RL learning, we have to choose how to model our problem.
There are 4 components that we would have to decide when modelling our problem.
These are:
S  - The state
A  - The set of actions which can be chosen at a given point
R  - The reward from performing action ‘a’ from set A
S’ - The new state after action ‘a’</p>
<p>I propose the following environment:
S  - The current “state” of the program which starts from freshly raised LLVM
A  - The set of optimization passes which we have written to be used
R  - The delta between the current similarity of the program to the original</p>
<blockquote>
<div>and the similarity after applying the optimization pass, action ‘a’, from
our set of optimizations, “A”</div></blockquote>
<p>S’ - The new state of the program after the optimization</p>
</div>
<div class="section" id="what-type-of-rl">
<h3>What type of RL<a class="headerlink" href="#what-type-of-rl" title="Permalink to this headline">¶</a></h3>
<p>In order to use the model I mention in the previous section, the algorithm we choose
would need to be able to handle problems that are “episodic” in nature. That is to say,
each time that we start learning, we pick a single program (llvm module, function, whatever)
and perform the learning on that. Then we use the new information learned from that
to improve results on the next section of code when we begin again. (Or do some level
of hybrid batch based learning where we are slightly off policy as we learn a large batch
and perform computations based on the average result. This could cause faster learning
by smoothing the results. This is why most neural nets used batch based learning)</p>
<p>I concluded that the best choice of RL learning would be a model free on-policy
based algorithm which is episodic in nature and has a finite space horizon. I also noted
that it would be nice to find a learning model which is able to learn well from
similarities amongst states. Data efficiency is less of an issue as I believe we
have a good amount of data.</p>
<div class="section" id="temporal-difference-learning">
<h4>Temporal Difference Learning<a class="headerlink" href="#temporal-difference-learning" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="temporal-difference-learning-with-back-prop">
<h4>Temporal Difference Learning with Back Prop<a class="headerlink" href="#temporal-difference-learning-with-back-prop" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="monte-carlo-learning">
<h4>Monte Carlo learning<a class="headerlink" href="#monte-carlo-learning" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="1_ReinforcementLearning-Zach.html" class="btn btn-neutral" title="Reinforcement Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Joshua Goralczyk, Salman Ahmed, Vatricia Edgar, YiZhuang Garrard, Zachary Monroe

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>