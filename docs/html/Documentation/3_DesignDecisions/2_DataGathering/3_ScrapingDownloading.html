

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scraping and Downloading Files from GitHub using GitHubScraper &mdash; DecomPy 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/extend_rtd.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Filtering the Files" href="4_FilterFiles.html" />
    <link rel="prev" title="Filtering the Repositories" href="2_RepoFiltering.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> DecomPy
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../1_Installation/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Design Decisions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../1_DesignAndArchitecture/index.html">Design and Architecture</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Data Gathering</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="1_DataGatheringOverview.html">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="2_RepoFiltering.html">Filtering the Repositories</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Scraping and Downloading Files from GitHub using GitHubScraper</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#relevant-classes">Relevant Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#relevant-methods">Relevant Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-use">How To Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-does-githubscraper-scrape">How does GitHubScraper Scrape?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-does-githubscraper-deal-with-files">How Does GitHubScraper Deal With Files?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-does-githubscraper-multithread-work">How Does GitHubScraper Multithread work?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#what-is-the-performance-of-githubscraper">What is the Performance of GitHubScraper?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-is-githubscraper-tested">How Is GitHubScraper Tested?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="4_FilterFiles.html">Filtering the Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="5_llvmGen.html">Generating LLVM IR from C Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="6_clangSubproc.html">Compiling C Source to Various Formats</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../3_Database/index.html">Database</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_BinaryLifting/index.html">Binary Lifting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_UsingAnExistingDecompiler/index.html">Using an Existing Decompiler</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../4_DecompilerInformation/index.html">Decompiler Information</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DecomPy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Design Decisions</a> &raquo;</li>
        
          <li><a href="index.html">Data Gathering</a> &raquo;</li>
        
      <li>Scraping and Downloading Files from GitHub using GitHubScraper</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Documentation/3_DesignDecisions/2_DataGathering/3_ScrapingDownloading.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="scraping-and-downloading-files-from-github-using-githubscraper">
<h1>Scraping and Downloading Files from GitHub using GitHubScraper<a class="headerlink" href="#scraping-and-downloading-files-from-github-using-githubscraper" title="Permalink to this headline">¶</a></h1>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Authors and Last Update:</th></tr>
<tr class="field-odd field"><td>&#160;</td><td class="field-body"><div class="first last line-block">
<div class="line">YiZhuang Garrard, November 20, 2018</div>
<div class="line"><a class="reference external" href="mailto:ygarrar1&#37;&#52;&#48;asu&#46;edu">ygarrar1<span>&#64;</span>asu<span>&#46;</span>edu</a></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#overview" id="id1">Overview</a></li>
<li><a class="reference internal" href="#relevant-classes" id="id2">Relevant Classes</a></li>
<li><a class="reference internal" href="#relevant-methods" id="id3">Relevant Methods</a></li>
<li><a class="reference internal" href="#how-to-use" id="id4">How To Use</a></li>
<li><a class="reference internal" href="#how-does-githubscraper-scrape" id="id5">How does GitHubScraper Scrape?</a><ul>
<li><a class="reference internal" href="#entry-point" id="id6">Entry Point</a></li>
<li><a class="reference internal" href="#getting-page-contents" id="id7">Getting Page Contents</a></li>
<li><a class="reference internal" href="#extracting-urls-from-page-content" id="id8">Extracting URLs from Page Content</a></li>
<li><a class="reference internal" href="#filtering-useful-urls-from-urls" id="id9">Filtering Useful URLs from URLs</a></li>
<li><a class="reference internal" href="#validating-urls" id="id10">Validating URLs</a></li>
<li><a class="reference internal" href="#going-through-all-repository-directories" id="id11">Going Through All Repository Directories</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-does-githubscraper-deal-with-files" id="id12">How Does GitHubScraper Deal With Files?</a><ul>
<li><a class="reference internal" href="#downloading-files" id="id13">Downloading Files</a></li>
<li><a class="reference internal" href="#storing-files" id="id14">Storing Files</a></li>
<li><a class="reference internal" href="#updating-json-file" id="id15">Updating json file</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-does-githubscraper-multithread-work" id="id16">How Does GitHubScraper Multithread work?</a></li>
<li><a class="reference internal" href="#what-is-the-performance-of-githubscraper" id="id17">What is the Performance of GitHubScraper?</a></li>
<li><a class="reference internal" href="#how-is-githubscraper-tested" id="id18">How Is GitHubScraper Tested?</a></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id1">Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This page covers the though process of how GitHub repositories are scraped, how files are downloaded and stores, and
other miscellaneous things that a person trying to figure out what’s going on would find useful.</p>
</div>
<div class="section" id="relevant-classes">
<h2><a class="toc-backref" href="#id2">Relevant Classes</a><a class="headerlink" href="#relevant-classes" title="Permalink to this headline">¶</a></h2>
<p>The classes that pertain to this section are GitHubScraper and it’s parent, WebNavigator. Zachary Monroe made the
WebNavigator class, and I just piggybacked off of it because it works. It’s also well commented. I did have to
modify it a bit so that when thing’s don’t work as intended I could fix it. The content on this page will all relate
to GitHubScraper.</p>
</div>
<div class="section" id="relevant-methods">
<h2><a class="toc-backref" href="#id3">Relevant Methods</a><a class="headerlink" href="#relevant-methods" title="Permalink to this headline">¶</a></h2>
<p>The only method that anybody not maintaining or improving on GitHubScraper is</p>
<blockquote class="highlights">
<div>download_all_file(repo_urls, target_directories=None)</div></blockquote>
<p>If you are maintaining or improving this class, then everything will be covered further down.</p>
</div>
<div class="section" id="how-to-use">
<h2><a class="toc-backref" href="#id4">How To Use</a><a class="headerlink" href="#how-to-use" title="Permalink to this headline">¶</a></h2>
<p>Pass in equally-long lists of GitHub repository URLs and directories to download into into</p>
<blockquote class="highlights">
<div>GitHubScraper.download_all_files(repo_urls, target_directories=None)</div></blockquote>
<p>If repo_urls is longer than 1, than target_directories must be specified.</p>
<p>Valid examples of calling this are:</p>
<blockquote class="highlights">
<div><p>GitHubScraper.download_all_files(“<a class="reference external" href="https://github.com/hexagon5un/AVR-Programming">https://github.com/hexagon5un/AVR-Programming</a>”, “Medium sized repo”)</p>
<p>GitHubScraper.download_all_files(“<a class="reference external" href="https://github.com/hexagon5un/AVR-Programming">https://github.com/hexagon5un/AVR-Programming</a>”)</p>
<p>GitHubScraper.download_all_files([“<a class="reference external" href="https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter19_EEPROM">https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter19_EEPROM</a>”], “FolderA”)</p>
<p>GitHubScraper.download_all_files([“<a class="reference external" href="https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter19_EEPROM">https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter19_EEPROM</a>”,”<a class="reference external" href="https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter06_Digital-Input">https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter06_Digital-Input</a>”], [“FolderA”, “FolderB”])</p>
<p>GitHubScraper.download_all_files(“<a class="reference external" href="https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter19_EEPROM/vigenereCipher">https://github.com/hexagon5un/AVR-Programming/tree/master/Chapter19_EEPROM/vigenereCipher</a>”)</p>
</div></blockquote>
</div>
<div class="section" id="how-does-githubscraper-scrape">
<h2><a class="toc-backref" href="#id5">How does GitHubScraper Scrape?</a><a class="headerlink" href="#how-does-githubscraper-scrape" title="Permalink to this headline">¶</a></h2>
<p>Carefully. Rather, I tried to be careful, but I’m sure you can muck it up.</p>
<div class="section" id="entry-point">
<h3><a class="toc-backref" href="#id6">Entry Point</a><a class="headerlink" href="#entry-point" title="Permalink to this headline">¶</a></h3>
<p>The entry point of the scraping process is when a user passes in two lists into</p>
<blockquote class="highlights">
<div>GitHubScraper.download_all_files(repo_urls, target_directories=None).</div></blockquote>
<p>The first list is a list of URLs to GitHub repositories. The URL can lead to the top level of the repository, or any
subdirectory within the repository, and only files that are in the directory and children directory will be downloaded.
The second list if the names of the target directories that files will be downloaded into. I did this because other
parts of the data gathering step are easier if specific names can be specified. The two lists must be equal length: If
they are not the same lengths, then it doesn’t make too much sense. For example, if there were more repository URLs and
target directory addresses, then where should the excess files from the extra repositories go? I could generate names
for them, but decided that it would be better not to do anything rather than download files into a directory which
nobody knows the name of. Conversely, if the target directories list is longer than the repo URLs list, what should I
do? I decided not to think too hard about it, and just enforced that the lengths of the two arguments have to be the
same.</p>
<p>If the two arguments are just strings, they are converted to lists with one item in them.</p>
<p>To summarize, the user passes in a list of GitHub repository URLs and an equally-long list of directories to download
files into. The files from each repository will be downloaded into the directory in the same index in the directory
list.</p>
</div>
<div class="section" id="getting-page-contents">
<h3><a class="toc-backref" href="#id7">Getting Page Contents</a><a class="headerlink" href="#getting-page-contents" title="Permalink to this headline">¶</a></h3>
<p>Look at</p>
<blockquote class="highlights">
<div>WebNavigator.getContent(link)</div></blockquote>
<p>The program sends out a request, and gets the page back. Most of the time.
If the same GitHub URL is requested too many times, the GitHub server will give you one of the HTTP errors about
sending too many requests.</p>
</div>
<div class="section" id="extracting-urls-from-page-content">
<h3><a class="toc-backref" href="#id8">Extracting URLs from Page Content</a><a class="headerlink" href="#extracting-urls-from-page-content" title="Permalink to this headline">¶</a></h3>
<p>Look at</p>
<blockquote class="highlights">
<div>WebNavigator.getAbsoluteLinksFromPage(link, domain=None)</div></blockquote>
<p>I actually don’t know how it works, so if you want to know, you actually have to look at it.</p>
</div>
<div class="section" id="filtering-useful-urls-from-urls">
<h3><a class="toc-backref" href="#id9">Filtering Useful URLs from URLs</a><a class="headerlink" href="#filtering-useful-urls-from-urls" title="Permalink to this headline">¶</a></h3>
<p>The URLs that are useful are the ones that lead to subdirectories within the repository as well as links to C files.
There are two separate lists to store these URLs, and they are extended every time a page is scraped and URLs
extracted. The method is</p>
<blockquote class="highlights">
<div>GitHubScraper.__scrape_page_urls(url).</div></blockquote>
<p>The first list is called subfolder_links and it holds the URLs to subdirectories. It is extended with every URL that has
the substring “/tree/master/”, doesn’t have “#”, isn’t the current page URL, and the length of the URL is longer that
the URL of the current page. “/tree/master/” makes sure that any URL that is selected is within the master branch. The
“tree” portion of that subsection indicates that it is a directory rather than a file. “#“‘s are excluded because they
are just specific sections of the current page. The current page URL is excluded because otherwise the scraping process
would infinitely loop. Scraped URLs have to be longer than the current page URL to ensure that any save URL does not
lead to a parent of the current page, thereby creating an infinite loop of scraping.</p>
<p>The second list is called file_links and it holds the links to files that will be downloaded. It is extended with every
URL that has the substring “/blob/master/” and ends with “.c”. “blob” from “/blob/master/” indicates that the URL
leads to a file rather than a directory, and “master” indicates that the file is on the master branch. Since the file
ends with “.c”, it’s a C file.</p>
</div>
<div class="section" id="validating-urls">
<h3><a class="toc-backref" href="#id10">Validating URLs</a><a class="headerlink" href="#validating-urls" title="Permalink to this headline">¶</a></h3>
<p>There is no purposely-built validation for URLs. If the user provides a bad URL, then the result is their problem,
not mine. I actually haven’t tried putting in a URL to someplace that is not a GitHub repository, but I suspect that
the program will eventually stop without destroying your directory tree. Just don’t put in bad URLs.</p>
<p>With that said, there is some unintentional URL validation built in when scraping is underway, which is the filter
described in <a class="reference internal" href="#filtering-useful-urls-from-urls">Filtering Useful URLs from URLs</a>. This makes is difficult to find URLs that this program will use if any
URL is passed as an argument in the <a class="reference internal" href="#entry-point">Entry Point</a>.</p>
</div>
<div class="section" id="going-through-all-repository-directories">
<h3><a class="toc-backref" href="#id11">Going Through All Repository Directories</a><a class="headerlink" href="#going-through-all-repository-directories" title="Permalink to this headline">¶</a></h3>
<p>Because of how URLs are extracted and filtered, as described in <a class="reference internal" href="#filtering-useful-urls-from-urls">Filtering Useful URLs from URLs</a> and
<a class="reference internal" href="#filtering-useful-urls-from-urls">Filtering Useful URLs from URLs</a>, it is (not mathematically) guaranteed that the directory and every child directory
from the argument URL will be traversed. I initially thought that I would do a breadth-first search of the directory
tree, but because I implemented multithreading, there’s no real order to how the directories are traversed.</p>
</div>
</div>
<div class="section" id="how-does-githubscraper-deal-with-files">
<h2><a class="toc-backref" href="#id12">How Does GitHubScraper Deal With Files?</a><a class="headerlink" href="#how-does-githubscraper-deal-with-files" title="Permalink to this headline">¶</a></h2>
<div class="section" id="downloading-files">
<h3><a class="toc-backref" href="#id13">Downloading Files</a><a class="headerlink" href="#downloading-files" title="Permalink to this headline">¶</a></h3>
<p>After getting the file URLs from <a class="reference internal" href="#filtering-useful-urls-from-urls">Filtering Useful URLs from URLs</a>, tuples are created that are made of three things:
the file name, which is retrieved from the last token of each URL when delimiting by “/”; the first URL within the page
that has the substring “raw” is used to request the page content from the GitHub server; and the content of the response
from the server. Content is retrieved as described in <a class="reference internal" href="#getting-page-contents">Getting Page Contents</a>. The method is called
GitHubScraper.__download_file(file_page_link), where file_page_link is the tuple described.</p>
</div>
<div class="section" id="storing-files">
<h3><a class="toc-backref" href="#id14">Storing Files</a><a class="headerlink" href="#storing-files" title="Permalink to this headline">¶</a></h3>
<p>Using the target directory as described in <a class="reference internal" href="#entry-point">Entry Point</a>, the import os is used to create a directory at that
directory if one does not already exist, creates a subdirectory call “C_files” within it, and writes a file using
the file name and content described in the tuple in <a class="reference internal" href="#downloading-files">Downloading Files</a>. Occasionally, there
is a UnicodeEncodeError, but I just print the error and ignore it because I don’t want to do research on that.
The method is called GitHubScraper.__file_content_into_storage(content_url_tuple, target_directory), where
content_url_tuple is the tuple, and target_directory is the directory to store the file.</p>
</div>
<div class="section" id="updating-json-file">
<h3><a class="toc-backref" href="#id15">Updating json file</a><a class="headerlink" href="#updating-json-file" title="Permalink to this headline">¶</a></h3>
<p>Only the target directory is needed to update the json file within that directory. The relevant method is
GitHubScraper.__update_meta(target_directory). If the directory doesn’t exist, that means that there were no C files
to download, so it just returns. If the directory does exist, it is checked to see if the json file exists. If it
exists, then update it using the datetime package, and if it doesn’t exist, create the file. The date is written
in YYYY-MM-DD HH:MM:SS format so that the database can query it or something. I don’t actually know how databases work.</p>
</div>
</div>
<div class="section" id="how-does-githubscraper-multithread-work">
<h2><a class="toc-backref" href="#id16">How Does GitHubScraper Multithread work?</a><a class="headerlink" href="#how-does-githubscraper-multithread-work" title="Permalink to this headline">¶</a></h2>
<p>Because scraping is a highly IO dependent process, it is better to utilize threads rather than processes.</p>
<p>GitHubScraper uses ThreadPoolExecutor to manage all the threads and futures. There are three sources of futures:
subfolder_links, file_links, and file_name_url_content_tuples. subfolder_links is a list of links that need to be
scraped. file_links is a list of links to files that need to be downloaded. file_name_url_contents_tuples are tuples
that are downloaded file information but still needs to be stored. Elements from subfolder_links are popped into
GitHubScraper.__scape_page_urls as a future, as are file_link elements into GitHubScraper.__download_file and
file_name_url_content_tuples into GitHubScraper.__file_content_into_storage. subfolder_links and file_links are
populated from <a class="reference internal" href="#filtering-useful-urls-from-urls">Filtering Useful URLs from URLs</a>. file_name_url_content_tuples is populated from <a class="reference internal" href="#downloading-files">Downloading
Files</a>. The number of workers is set as the default, since I couldn’t find any compelling reason to change that.
I limited the number of futures to have a minimum ceiling of 250, and maximum ceiling of the longest length between
subfolder_links and file_links. The maximum ceiling is enforced by only allowing elements of subfolder_links or
file_links to be popped off for processing when the length of one of them is less than the number of futures that
are still not complete. I do this for performance reasons, since I don’t want to have any large data structure
taking up time reallocating memory, and keeping everything bounded to the same maximum minimizes the number of
times memory has to be reallocated. Elements in file_name_url_content_tuples are submitted for processing as fast
as possible to minimize latency of the entire program. I don’t want to file everything towards the end of the
scraping process because I think it’s faster to do it the way I am. It also keeps file_name_url_content_tuples to a
minimum, since it has the potential to be huge due it each tuple holding all the text of a file in addition to the
file name and the URL.</p>
</div>
<div class="section" id="what-is-the-performance-of-githubscraper">
<h2><a class="toc-backref" href="#id17">What is the Performance of GitHubScraper?</a><a class="headerlink" href="#what-is-the-performance-of-githubscraper" title="Permalink to this headline">¶</a></h2>
<p>About 100 kB/s of pure C code. This was measured by scraping through <a class="reference external" href="https://github.com/torvalds/linux">Linux</a>. So
it’s pretty slow.</p>
</div>
<div class="section" id="how-is-githubscraper-tested">
<h2><a class="toc-backref" href="#id18">How Is GitHubScraper Tested?</a><a class="headerlink" href="#how-is-githubscraper-tested" title="Permalink to this headline">¶</a></h2>
<p>Unit tests. I really need to put in more tests. They’re not comprehensive right now.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="4_FilterFiles.html" class="btn btn-neutral float-right" title="Filtering the Files" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="2_RepoFiltering.html" class="btn btn-neutral" title="Filtering the Repositories" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Joshua Goralczyk, Salman Ahmed, Vatricia Edgar, YiZhuang Garrard, Zachary Monroe

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>