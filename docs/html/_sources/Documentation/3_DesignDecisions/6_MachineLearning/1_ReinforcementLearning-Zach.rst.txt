Reinforcement Learning
======================


Types of RL
-----------

Model free vs. Model based learning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Model free and model based learning are simple in concept.
In model based learning, the RL agent generates a model of the
world as it learns. In model free learning, the agent learns
"notions" of general things to avoid and things to try for.

I don't believe that a model based learning would be as useful
for what we are trying to do. Model based learning is really not
intended for such complex statesets. Even for learning to play
board games like backgammon, model based learning has trouble
keeping up with the state space. Depending on how we choose to
model state in our problem, model based learning might actually
be impossible.

Currently, I think it makes more sense to
model each set of code as an environment and have the agent
learn how to optimize in that environment with the notion that
improvements done in one environment apply to others.

::

    After further reading, I don't think that the state model I
    mentioned above is the best idea. See "Reinforcement Learning
    Ideas" for more information.


On-policy vs. Off-policy
~~~~~~~~~~~~~~~~~~~~~~~~

Off-policy learning is essentially the idea of having the RL
agent perform actions completely independently of what it is
learning.

In contrast, on policy learning has the agent perform tasks,
learn from the results, and then perform new actions based
on what it learnt.

The advantage of off-policy learning (since it is likely not
self evident) is that by using off-policy learning, an RL agent
does not become biased into avoiding the choices from which it
could learn from. For example, if an agent was trying to solve
a maze, and it learned early on that going right gave good
results, it might never learn that going left at that turn
may have bad results at first but later on have a massive
reward that was "hidden" so to speak.

I think that despite off-policy learning's advantages, on-policy
learning is still a better choice for what we are doing, mostly
due to the fact that off-policy learning tends to be much slower.

Finite vs Infinite Space Horizons
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A space horizon in RL can be thought of as simply the time that
an agent has to live. The most common type in RL is likely an
"infinite space horizon." This means that an agent can live
forever and should try to plan ahead as far as possible to be 
successful. Of course, pure infinite space horizon problems are
rare. This is becuase of an issue that arises where an agent
might put of reward far too much such that it never improves
if it is aware of a growing reward. This situation is fixed
through a concept called "discounted inifinte space horizons."
This simply means that a diminishing return is placed on
future returns so that an agent will tend towards improvement.

I think that both finite space horizons and discounted infinite
space horizons have applications for our problem. Due to the
execution times of llvm passes however, I think a finite space
horizon could be better. This is since it could be used to
arbitrarily limit the number of passes that the agent could make
preventing it from trying to continually trying to run something 
or minor improvement at the cost of huge runtime.

RL vs. Neural Nets
------------------

Neural Net based learning uses a simple idea. The neural network
guesses what output should come from a given input. Using the
correct output, and a technique called backpropogation, the error
is "brough back" into what features caused it. These weights given
to these features can then be adjusted until the AI is able to more
correctly guess the outputs.

Reinforcement learning revolves around an "agent" making decisions
in an "environment". Essentially, the agent makes choices and by
doing so receives a reward. The magnitude of this reward (possibly
negative) helps it to learn what actions to take.

Both NN and RL try to get a machine learning agent to move closer
to a desired result. NN do so through using a scalar output and
a desired scalar output and determining the cause of the difference.
RL uses a "reward function" which could be but does not have to be
derived from the desired output.

Less simply, in RL, there are 4 major factors. These are the state
of the environment, the set of actions available for the agent to
choose from. Each of these would have to be modelled for our problem.

One big advantage of neural nets, to quote a `source I found on RL
<https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html#x26-1350009.3>`_
"is, of course, their ability to generalize learning across similar states."
This advantage will be greatly missed should be choose to go with
RL (unless we strategically chose the right algorithm -- this excerpt
is from a section on Temporal Difference learning with Back Prop).

