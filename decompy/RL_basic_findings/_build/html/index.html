
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to RL Basic Findings’s documentation! &#8212; RL Basic Findings 1.0.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="welcome-to-rl-basic-findings-s-documentation">
<h1>Welcome to RL Basic Findings’s documentation!<a class="headerlink" href="#welcome-to-rl-basic-findings-s-documentation" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>A summary of A Beginner’s Guide to Deep Reinforcement Learning</p>
<p>Link: <a class="reference external" href="https://skymind.ai/wiki/deep-reinforcement-learning">https://skymind.ai/wiki/deep-reinforcement-learning</a></p>
<p>Reinforcement learning refers to goal-oriented algorithms, which learn how to attain
a complex objective (goal) or maximize along a particular dimension over many steps.
Like a child incentivized by spankings and candy, these algorithms are penalized when
they make the wrong decisions and rewarded when they make the right ones – this is
reinforcement. Reinforcement learning can be understood using the concepts of agents,
environments, states, actions and rewards.</p>
<p>An agent takes actions. A is the set of all possible moves the agent can make. An agent
can make an action from a list of actions. Environment is the world through which the
agent moves. Unlike other forms of machine learning – such as supervised and unsupervised
learning – reinforcement learning can only be thought about sequentially in terms of
state-action pairs that occur one after the other.</p>
<p>Reinforcement learning judges actions by the results they produce. It is goal oriented,
and its aim is to learn sequences of actions that will lead an agent to achieve its goal,
or maximize its objective function. Reinforcement learning differs from both supervised
and unsupervised learning by how it interprets inputs.</p>
<p>Unsupervised learning: That thing is like this other thing; perform anomaly detection by
recognizing what is unusual or dissimilar.</p>
<p>Supervised learning: These algorithms learn the correlations between data instances and
their labels; that is, they require a labelled dataset.</p>
<p>Reinforcement learning: Eat that thing because it tastes good and will keep you alive longer.
The goal of reinforcement learning is to pick the best known action for any given state,
which means the actions have to be ranked, and assigned values relative to one another.</p>
<p>Reinforcement learning is iterative. In its most interesting applications, it doesn’t begin
by knowing which rewards state-action pairs will produce. It learns those relations by running
through states again and again, like athletes or musicians iterate through states in an attempt
to improve their performance.</p>
<p>Neural networks are the agent that learns to map state-action pairs to rewards.</p>
</div></blockquote>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Salman Ahmed.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.7</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.8</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>