
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to Applying RL to the Adaptive Decompiler’s documentation! &#8212; Applying RL to the Adaptive Decompiler 1.0.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="welcome-to-applying-rl-to-the-adaptive-decompiler-s-documentation">
<h1>Welcome to Applying RL to the Adaptive Decompiler’s documentation!<a class="headerlink" href="#welcome-to-applying-rl-to-the-adaptive-decompiler-s-documentation" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><blockquote>
<div>Simple Reinforcement Learning with TensorFlow</div></blockquote>
<p>Link: <a class="reference external" href="https://medium.com/&#64;awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149">https://medium.com/&#64;awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149</a></p>
<p>Reinforcement learning provides the capacity for us not only to teach an artificial
agent how to act, but to allow it to learn through its own interactions with an environment.</p>
<p>RL algorithms must enable the agent to learn the correct pairings itself through the use
of observations, rewards, and actions. This can be applied to the adaptive decompiler’s
machine learning agent.</p>
<p>Since a piece of code can range anywhere from, for instance, a simple for-loop to complex ADTs,
feeding user selected information to the agent would not be an ideal way for machine learning.
We must implement a RL algorithm that allows the code to learn correct pairings itself through
the use of observations, rewards, and actions.</p>
<p>Different actions yield different rewards. Selecting the if condition, for example, in a program
may lead to the user specified correct answer. Conversely, selecting the else condition may lead
to the user specified incorrect answer.</p>
<p>Rewards are delayed over time. Even if selecting the if condition may be the right choice,
the result may not be known until the program is finished executing.</p>
<p>Reward for an action is conditional on the state of the environment. Selecting the if condition
may be the right decision sometimes. However, it may not be the correct decision in some other
circumstances.</p>
<p>An RL algorithm allows us to increase the weight for actions that yields a positive reward and
decrease them for actions that yields a negative reward. In this way the agent will be more or
less likely to pick that action in the future.</p>
<p>In the ML agent component of the Adaptive decompiler, a RL algorithm can be used. If a piece of
low-level code is successfully translated to C source code, the weight that yielded that outcome
might be high. Conversely, the weight might be low for an action that does a poor job in translating
the low-level code to C code. Using these weights and the differences between them, the ML agent
can learn what actions to take in the future so that it can achieve outcomes that are high.</p>
</div></blockquote>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Salman Ahmed.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.7</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.8</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>